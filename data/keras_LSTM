# This code is used to train a LSTM model to predict the occupancy of cars in the SJSU parking garages, this is done using a dataset of density values and time
# This Code only uses CPU, if you want to use GPU you will need to install the GPU version of tensorflow, Use linux, and install the funny special cuda drivers,
# don't bother and just run it on CPU until you have a good reason to use GPU

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import os
import keras_tuner as kt

# traing the final model more again using the best hyperparameters found by the hyperband tuner
perform_final_training = True


# predict x steps into the future 
future_steps = 36


# Create directory for saving graphs
output_dir = "epoch_results"
os.makedirs(output_dir, exist_ok=True)

# Load Data
data = pd.read_csv("log.csv") 

# Drop unnecessary columns
data = data.drop(columns=["Unnamed: 0", 'south density', 'west density', 'north density', 'south compus density'])

# Convert time to datetime
timestamps = pd.to_datetime(data.iloc[:, 0])

#Cyclical Time Encoding
data['month_sin'] = np.sin(2 * np.pi * timestamps.dt.month / 12)
data['month_cos'] = np.cos(2 * np.pi * timestamps.dt.month / 12)
data['day_sin'] = np.sin(2 * np.pi * timestamps.dt.day / 31)
data['day_cos'] = np.cos(2 * np.pi * timestamps.dt.day / 31)
data['day_of_week_sin'] = np.sin(2 * np.pi * timestamps.dt.dayofweek / 7) * 2
data['day_of_week_cos'] = np.cos(2 * np.pi * timestamps.dt.dayofweek / 7) * 2
data['hour_sin'] = np.sin(2 * np.pi * timestamps.dt.hour / 24) * 2
data['hour_cos'] = np.cos(2 * np.pi * timestamps.dt.hour / 24) * 2
data['minute_sin'] = np.sin(2 * np.pi * timestamps.dt.minute / 60)
data['minute_cos'] = np.cos(2 * np.pi * timestamps.dt.minute / 60)

# Drop original time columns
data = data.drop(columns=[data.columns[0]])

data = data[:-625] # Remove last x rows for testing, change this value to try and find different data sets to validate the model with


# Train-test split
train_size = int(len(data) * 0.95) # 95% train, 5% test, I don't recommend increase this alot, as the model will get confused by the winter data
train_data = data.iloc[:train_size]
test_data = data.iloc[train_size:]

scaler = MinMaxScaler() # Scalers are used to normalize the data, this makes it easier for the model to learn the data
scaler.fit(train_data) # Fit the scaler to the training data

# Transform the data using the scaler
train_scaled = pd.DataFrame(scaler.transform(train_data), columns=train_data.columns)
test_scaled = pd.DataFrame(scaler.transform(test_data), columns=test_data.columns)

# Sequence Settings, the sequence length is the size of the rolling window that we use to make predictions
seq_size = 64  # Adjust this value to experiment with different sequence lengths, longer seqeunce sizes produce more accurate predictions, but increase the training time exponentially
n_feature = data.shape[1] # n_features is the number of inputs to the model, if we want to give the model more context we can increase this value

# Batch size is the number of samples that are processed before the model is updated
# a larger batch size means that the model will be updated less frequently, but each update will be more accurate
# larger batch sizes require more memory but run faster and are less accurate
# smaller batch sizes require less memory but run much slower are more accurate
keras_batch_size = 64

# Positional encoding is used to give the model information about the position of the data in the sequence
# You don't need to know how this works and I don't either
def add_positional_encoding(data, seq_size):
    position = np.arange(seq_size)[:, np.newaxis]
    div_term = np.exp(np.arange(0, n_feature, 2) * -(np.log(10000.0) / n_feature))
    pos_enc = np.zeros((seq_size, n_feature))
    pos_enc[:, 0::2] = np.sin(position * div_term)
    pos_enc[:, 1::2] = np.cos(position * div_term)
    return data + pos_enc

# time series generators are used to create the rolling windows of data that we use to train the model, the length of the windows is seq_size
train_generator = keras.preprocessing.sequence.TimeseriesGenerator(
    train_scaled.values, train_scaled.values[:, :n_feature], length=seq_size, batch_size=keras_batch_size)

test_generator = keras.preprocessing.sequence.TimeseriesGenerator(
    test_scaled.values, test_scaled.values[:, :n_feature], length=seq_size, batch_size=keras_batch_size)

# Hyperparameter Tuning Model
# This model is used to search for the best hyperparameters for the LSTM model, this is done using the keras hyperbadn tuner
def build_model(hp):
    # lstm_units is the number of units in the LSTM layer, this is the number of neurons in the layer
    lstm_units = hp.Int('lstm_units', 1, 256, step=1) 

    # num_lstm_layers is the number of LSTM layers in the model, this adds depth to the model which can help it learn more complex patterns, but will decrease training time and may cause overfitting
    num_lstm_layers = hp.Int('num_lstm_layers', 1, 3, step=1) 

    # dropout_rate is the rate at which neurons are randomly dropped out of the model, this is used to prevent overfitting
    dropout_rate = hp.Float('dropout_rate', 0.1, 0.5, step=0.1)

    # initial_learning_rate is the starting learning rate for the model, this is the rate at which the model learns, which has a large impact on the model's performance
    initial_learning_rate = hp.Choice('initial_learning_rate', [1e-1, 1e-2, 1e-3, 7.5e-4, 5e-4, 2.5e-4, 1e-4, 7.5e-5, 5e-5, 2.5e-5, 1e-5])

    # decay_steps is the number of steps before the learning rate is decayed, this is used to prevent the model from learning too quickly and causing the model to overfit
    decay_steps = hp.Int('decay_steps', 1000, 10000, step=1000) # decay_steps refer to the number training steps when running the training 
    decay_rate = hp.Float('decay_rate', 0.5, 0.999, step=0.001)

    inputs = keras.layers.Input(shape=(seq_size, n_feature))
    x = inputs

    for _ in range(num_lstm_layers):
        x = keras.layers.Bidirectional(keras.layers.LSTM(lstm_units, return_sequences=True))(x)
        x = keras.layers.Dropout(dropout_rate)(x)

    # Flatten the output of the LSTM layer, this is used to make the output of the LSTM layer compatible with the dense layer
    x = keras.layers.Flatten()(x)

    # The dense layer is used for final prediction of the model, this layer is used to make the final prediction
    # we use sigmoid to contrain outputs to between 0 and 1 
    outputs = keras.layers.Dense(n_feature, activation='sigmoid')(x)

    # Learning Rate Schedule, uses parameters from the hyperparameter search above
    lr_schedule = keras.optimizers.schedules.ExponentialDecay( 
        initial_learning_rate=initial_learning_rate,
        decay_steps=decay_steps,   
        decay_rate=decay_rate,
        staircase=True )

    # Build final model
    model = keras.Model(inputs=inputs, outputs=outputs)
    model.compile(
        # log cosh loss is used to prevent exploding gradients, I've found this is by far the best loss function, but you can try other ones
        loss=keras.losses.LogCosh(reduction="sum_over_batch_size", name="log_cosh", dtype=None), 
        # Lion is a custom optimizer that I've found works well for this model, but there are others you can try 
        optimizer=keras.optimizers.Lion(learning_rate=lr_schedule), 
        # show a second metric for clarity 
        metrics=[tf.keras.metrics.MeanSquaredError()])
    return model

# Hyperparameter Tuning, https://keras.io/keras_tuner/api/tuners/hyperband/
# Hyperband is a bandit-based method for hyperparameter optimization
# bandit-based means that it uses the information from previous runs to allocate resources to configurations that are more likely to perform well
# this will take a very long time to run so be careful what you put in as parameters 
tuner = kt.Hyperband(
    build_model, # build_model is the function that defines the model architecture and hyperparameters
    objective="val_loss",  # objective is the metric that the tuner uses to evaluate the model, this is the metric that the tuner tries to minimize
    max_epochs=50, # epochs is the maximum number of epochs to train each model
    hyperband_iterations=1, # hyperband_iterations is the number of times the search algorithm is run 
    factor=3, # factor is the resource allocation factor, which determines the number of configurations that are discarded in each iteration 
    directory="tuner_results",
    project_name="lstm_tuning",
    seed=123
)

# stops learning if val_loss does not improve for 25 epochs, val loss describes the loss on the validation set
early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)

# reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=5, min_lr=1e-6)
    # I don't need this function any more but im keeping it here anyway 

# This callback is used to make predictions at the end of each epoch, this is used to visualize the model's performance over the training time
class PredictionCallback(keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        if epoch % 1 == 0: # Change this value to change the frequency of the printouts

            # Ensure starting input is the last `seq_size` observations
            sample_batch = test_scaled.values[-(seq_size + future_steps):-future_steps].reshape(1, seq_size, n_feature)

            sliding_predictions = []
            current_input = sample_batch.copy()

            # Predict future steps using a sliding window approach
            for _ in range(future_steps):
                pred = self.model.predict(current_input, verbose=0)[0]
                sliding_predictions.append(pred)

                current_input[:, :-1, :] = current_input[:, 1:, :]
                current_input[:, -1, :] = pred  # Update with new prediction

            sliding_predictions = np.array(sliding_predictions)

            # Actual values for comparison
            actual_future = test_scaled.values[-future_steps:, :n_feature]

            # Apply inverse transformation
            unscaled_predictions = scaler.inverse_transform(sliding_predictions)[:, :n_feature]
            unscaled_actual = scaler.inverse_transform(actual_future)[:, :n_feature]

            # Plot results
            plt.figure(figsize=(12, 6))

            # Define correct time ranges
            context_length = unscaled_actual.shape[0] 
            future_length = unscaled_predictions.shape[0] 

            context_range = range(len(test_scaled) - seq_size, len(test_scaled) - seq_size + context_length)  # Context window
            future_range = range(len(test_scaled) - seq_size + context_length, len(test_scaled) - seq_size + context_length + future_length)  # Future predictions

            feature_colors = ['blue', 'orange', 'green', 'purple']
            line_styles = ['-', '--']

            for i in range(4):
                # Plot past observed data
                plt.plot(context_range, unscaled_actual[:, i], linestyle=line_styles[0],
                         color=feature_colors[i], label=f'Actual Feature {i+1}')

                # Plot predicted future data
                plt.plot(context_range, unscaled_predictions[:, i], linestyle=line_styles[1],
                         color=feature_colors[i], label=f'Predicted Feature {i+1}')

            plt.xlabel("Time Steps")
            plt.ylabel("Values")
            plt.title(f"Epoch {epoch} - Actual vs. Predicted Data")
            plt.legend()
            plt.grid(True)
            plt.savefig(f"{output_dir}/epoch_{epoch}.png")
            plt.close()

# Hyperparameter search
tuner.search(train_generator, validation_data=test_generator, epochs=50, callbacks=[early_stopping])

# Retrieve best model and show summary
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
best_model = tuner.hypermodel.build(best_hps)
print(best_model.summary())



if perform_final_training:
    history = best_model.fit(
        train_generator,
        validation_data=test_generator,
        epochs=100,
        callbacks=[early_stopping, PredictionCallback()])
    
    # Plot training loss 
    # good video on interpreting loss graph: https://www.youtube.com/watch?v=p3CcfIjycBA
    def plot_loss(history):
        plt.figure(figsize=(10, 5))
        plt.plot(history.history['loss'], label='Loss')
        plt.plot(history.history['val_loss'], label='Val Loss')
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.title('Training and Validation Loss Over Epochs')
        plt.legend()
        plt.show()
        plt.savefig(f"{output_dir}/training_loss.png")
        plt.close()
    plot_loss(history)

def make_final_prediction(model, test_scaled, scaler, seq_size, n_feature, output_dir="epoch_results"):
    # Ensure starting input is the last `seq_size` observations
    sample_batch = test_scaled.values[-(seq_size + future_steps):-future_steps].reshape(1, seq_size, n_feature)

    sliding_predictions = []
    current_input = sample_batch.copy()

    # Predict future steps using a sliding window approach
    for _ in range(future_steps):
        pred = model.predict(current_input, verbose=0)[0]
        sliding_predictions.append(pred)

        current_input[:, :-1, :] = current_input[:, 1:, :]
        current_input[:, -1, :] = pred  # Update with new prediction

    sliding_predictions = np.array(sliding_predictions)

    # Actual values for comparison
    actual_future = test_scaled.values[-future_steps:, :n_feature]

    # Apply inverse transformation
    unscaled_predictions = scaler.inverse_transform(sliding_predictions)[:, :n_feature]
    unscaled_actual = scaler.inverse_transform(actual_future)[:, :n_feature]

    # Plot results
    plt.figure(figsize=(12, 6))

    # Define correct time ranges
    context_length = unscaled_actual.shape[0]  # Should match sequence length
    future_length = unscaled_predictions.shape[0]  # Should match prediction steps

    context_range = range(len(test_scaled) - seq_size, len(test_scaled) - seq_size + context_length)  # Context window
    future_range = range(len(test_scaled) - seq_size + context_length, len(test_scaled) - seq_size + context_length + future_length)  # Future predictions

    feature_colors = ['blue', 'orange', 'green', 'purple']
    line_styles = ['-', '--']

    for i in range(4):
        # Plot past observed data
        plt.plot(context_range, unscaled_actual[:, i], linestyle=line_styles[0],
                 color=feature_colors[i], label=f'Actual Feature {i+1}')

        # Plot predicted future data
        plt.plot(context_range, unscaled_predictions[:, i], linestyle=line_styles[1],
                 color=feature_colors[i], label=f'Predicted Feature {i+1}')

    plt.xlabel("Time Steps")
    plt.ylabel("Values")
    plt.title("Final Prediction - Actual vs. Predicted Data")
    plt.legend()
    plt.grid(True)
    plt.savefig(f"{output_dir}/final_prediction.png")
    plt.close()

make_final_prediction(best_model, test_scaled, scaler, seq_size, n_feature, output_dir="epoch_results")
